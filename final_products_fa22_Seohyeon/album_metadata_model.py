# -*- coding: utf-8 -*-
"""album_metadata_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-5TktQcslNGd8rxeQMTeP_ymK8RUj1Hy

XGBoost model created as part of p-music (P-ai Fall 2022) by Seohyeon Lee
@seohyeon-lee-2025
"""

import pandas as pd
import numpy as np
#import re

#load dataset
ds = pd.read_csv('merged_features.csv')

ds.shape
ds = ds.drop('Unnamed: 0', axis = 1)
ds.columns

#merge data from AOTY and Metacritic
col_list = ['AOTY Critic Score', 'Metacritic User Score', 'AOTY User Score', 'Metacritic Critic Score', 'Metacritic User Reviews', 'AOTY User Reviews', 'AOTY Critic Reviews', 'Metacritic Reviews']
#replace missing data with 0
ds[col_list] = ds[col_list].fillna(0)
ds[:5]

critic_score = ['AOTY Critic Score', 'Metacritic Critic Score'] #if we incorporate info from more sites these lists will have more items
user_score = ['AOTY User Score', 'Metacritic User Score']
critic_reviews = ['AOTY Critic Reviews', 'Metacritic Reviews']
user_reviews = ['AOTY User Reviews', 'Metacritic User Reviews']
merge_list = [critic_score, user_score, critic_reviews, user_reviews]

#tried curried function but chain indexing became a problem
#uncurried version below
def scoreMergeUncurried(col_list):
  merged_score = []
  for x in ds.index:
    if ds.loc[x, col_list[0]]==0 and ds.loc[x, col_list[1]]==0 : #both values filled with 0
     merged_score.append(sum(merged_score)/len(merged_score)) #switch to substitution
    elif ds.loc[x, col_list[0]]==0:
      merged_score.append(ds.loc[x, col_list[1]])
    elif ds.loc[x, col_list[1]]==0:
      merged_score.append(ds.loc[x, col_list[0]])
    else: #neither missing -> use mean
      merged_score.append((ds.loc[x, col_list[0]]+ds.loc[x, col_list[0]])/2)
  return merged_score

ds['merged_critic_score'] = scoreMergeUncurried(critic_score)
ds['merged_user_score'] = scoreMergeUncurried(user_score)
ds['merged_critic_reviews'] = scoreMergeUncurried(critic_reviews)
ds['merged_user_reviews'] = scoreMergeUncurried(user_reviews)

#drop original columns at once here
ds = ds.drop(col_list, axis=1)

#numberify(?) release year. 

ds['Release Year'] = ds['Release Date'].str.replace("-", "").apply(lambda x: x[-2:])
ds['Release Year'] = [x if x.isnumeric() else np.nan for x in ds['Release Year']]
ds['Release Year'] =  [x+2000 if x<23 and x != np.nan else x+1900 for x in ds['Release Year'].astype('int64')]
ds['Release Year'].isna().any()

#convert number of ratings into number data type
ds['Number of Ratings'] = ds['Number of Ratings'].str.replace(',', '').astype('int64')
#ds['Number of Ratings'].isna().any()

#drop release date and month and day (date: redundant with year, month and day have almost no correlation with output)
ds = ds.drop(['Release Date', 'Release Month', 'Release Day'], axis = 1)

#for each row in ds["Genre"]
#if value is missing
#replace with first value in ds["Genres"]

missing_genre = ds.loc[ds["Genre"].isna()]
for x in missing_genre.index:
  ds.loc[x, 'Genre'] = ds.loc[x, 'Genres'][0]

for x in ds.index:
  if "," in ds.loc[x, 'Genre']:
    ds.loc[x, 'Genre'] = ds.loc[x, 'Genre'].split(',')[0]

#cleaning categorical columns
def catClean(col):
  top_list = col.value_counts()[:20].index.tolist()
  col = col.fillna("None")
  new_col = col.apply(lambda row: row.split(', ')[0])
  new_col2 = new_col.map(lambda row: "Other" if (row not in set(top_list)) else row)
  return new_col2

ds['Descriptors'] = catClean(ds['Descriptors'])
ds["Descriptors"][:5]
#len(ds["Descriptors"].loc[lambda x: x=="Other"])
#More than half of descriptors don't belong to the top 20
#col = ds["Descriptors"].map(lambda row: "Other" if (row not in set(top_list)) else row) #this changes the value in the series
#col[:10]

ds["Label"] =  catClean(ds['Label'])
len(ds["Label"].loc[lambda x: x=="Other"])

ds.isna().any()

#data processing for XGBoost
from sklearn.model_selection import train_test_split

y = ds['Average Rating']
X = ds.drop(['Average Rating'], axis=1)

num_features = X.select_dtypes(include=np.number).columns
#print("Num_features: ",num_features)

cat_features = X.select_dtypes(include=object).columns
#print("cat_features: ",cat_features)

# Break off validation set from training data
X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,
                                                                random_state=0)

# "Cardinality" means the number of unique values in a column
# Select categorical columns with relatively low cardinality 
#decided not to use genres and descriptors because these have too many categories
#tried putting small categories into "Other" but more than half of the rows ended up in "other"
low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and 
                        X_train_full[cname].dtype == "object"]

# Select numeric columns
numeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]

# Keep selected columns only
my_cols = low_cardinality_cols + numeric_cols
X_train = X_train_full[my_cols].copy()
X_valid = X_valid_full[my_cols].copy()

# One-hot encode data
X_train = pd.get_dummies(X_train)
X_valid = pd.get_dummies(X_valid)
X_train, X_valid = X_train.align(X_valid, join='left', axis=1)

from xgboost import XGBRegressor

# Define the model
my_model = XGBRegressor(n_estimators = 550, random_state=0) # Your code here

# Fit the model
my_model.fit(X_train, y_train, early_stopping_rounds = 10, eval_set = [(X_valid, y_valid)]) # Your code here

from sklearn.metrics import mean_absolute_error

predictions = my_model.predict(X_valid)
mae = mean_absolute_error(predictions, y_valid)
mae

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

plt.scatter(predictions, y_valid, alpha = 0.5)
plt.xlabel("Predicted rating")
plt.ylabel("Actual rating")

plt.title("Predicted Album Ratings using XGBoost")

plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns
# classify into discrete classes
# build confusion matrix to compare with CNN output

#class boundaries from CNN code
class_list = [[0.627, 0.793], [0.793, 0.818], [0.818, 0.841], [0.841, 0.930]]

#function to assign class for each entry
def ratingScaler(row):
  for item in class_list:
    if item[0] <= (row/5) and (row/5) < item[1]:
      idx = class_list.index(item)
      return idx

predictions_series = pd.Series(predictions)
y_valid_series = pd.Series(y_valid)
    
predictions_rounded = predictions_series.apply(ratingScaler)
y_valid_rounded = y_valid_series.apply(ratingScaler)


#oscar's code from github
def get_confusion_matrix(output_targets, output_preds):
    # create a confusion matrix to illustrate results
    cm = confusion_matrix(output_targets, output_preds)
    #range: (minval, maxval_exclusive, steps)
    cm_df = pd.DataFrame(cm, index = list(range(0,4,1)), columns = list(range(0,4,1)))
    # compute accuracy
    accuracy = 0
    for i in range(len(cm)):
        for j in range(len(cm[i])):
            if i ==j:
                accuracy += cm[i][j]
    accuracy /= len(output_preds) # divide total correct by total obs
    cm_norm_df = cm_df / cm_df.sum() # divide each column by the sum for that column to determine relative precentage
    # plot
    plt.figure(figsize=(10,7))
    sns.heatmap(cm_norm_df, cmap = 'viridis', annot=True)
    plt.title('XGBoost results rounded into 4 classes, accuracy = %f'%np.round(accuracy, 4), fontsize=20)
    plt.ylabel('Actual variable class', fontsize=16)
    plt.xlabel('Predicted variable class', fontsize=16)
    #plt.savefig(os.path.join(DATA_DIR, 'confusion_acc_v0.0.1.jpeg'))
    plt.show()


get_confusion_matrix(predictions_rounded, y_valid_rounded)