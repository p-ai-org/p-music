{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9K3yJyAkc6s0S+xfhiRxB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/p-ai-org/p-music/blob/main/datapipeline_exploration_Seohyeon/bayesian_neural_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GGxeNFxan0I",
        "outputId": "32bf51b0-af58-4f8d-b97b-b64c25f0a01b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.7/dist-packages (0.17.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (0.1.7)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (1.5.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (4.4.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (1.21.6)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (1.3.0)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (0.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (4.6.0)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (3.19.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (4.64.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.21.6)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.10.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (4.1.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (5.10.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.3.6)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.9.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (1.24.3)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.7/dist-packages (from etils[epath]->tensorflow-datasets) (3.10.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.56.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-probability\n",
        "!pip install tensorflow-datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp"
      ],
      "metadata": {
        "id": "5I2GQLV-a686"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dimension of spectrograms\n",
        "image_size = (1001,512)\n",
        "batch_size = 128\n",
        "\n",
        "#process spectrograms\n",
        "#is this step necessary?\n",
        "train_ds, val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"enter directory\",\n",
        "    validation_split=0.2,\n",
        "    #return both train and val datasets\n",
        "    subset=\"both\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        ")"
      ],
      "metadata": {
        "id": "6JH_u-OxhlRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # We shuffle with a buffer the same size as the dataset.\n",
        "train_dataset = (\n",
        "        train_ds.take(train_size).shuffle(buffer_size=train_size).batch(batch_size)\n",
        "    )\n",
        "    test_dataset = dataset.skip(train_size).batch(batch_size)\n",
        "\n",
        "    return train_dataset, test_dataset\n"
      ],
      "metadata": {
        "id": "Vr-Q6y1DbJeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(model, loss, train_dataset, test_dataset):\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
        "        loss=loss,\n",
        "        metrics=[keras.metrics.RootMeanSquaredError()],\n",
        "    )\n",
        "\n",
        "    print(\"Start training the model...\")\n",
        "    model.fit(train_dataset, epochs=num_epochs, validation_data=test_dataset)\n",
        "    print(\"Model training finished.\")\n",
        "    _, rmse = model.evaluate(train_dataset, verbose=0)\n",
        "    print(f\"Train RMSE: {round(rmse, 3)}\")\n",
        "\n",
        "    print(\"Evaluating model performance...\")\n",
        "    _, rmse = model.evaluate(test_dataset, verbose=0)\n",
        "    print(f\"Test RMSE: {round(rmse, 3)}\")"
      ],
      "metadata": {
        "id": "CKk08Mu0b89O"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spectrograms 2262*512 or 1001*512\n",
        "dataset_size = #enter dataset size\n",
        "batch_size = 256\n",
        "num_epochs = 100\n",
        "sample = 10\n",
        "train_size = int(dataset_size * 0.85)\n",
        "\n",
        "mse_loss = keras.losses.MeanSquaredError()\n",
        "\n",
        "# Split the datasets into two groups\n",
        "train_dataset, test_dataset = get_train_and_test_splits(train_size, batch_size)\n",
        "\n",
        "# Sample some example targets from the test dataset\n",
        "examples, targets = list(test_dataset.unbatch().shuffle(batch_size * 10).batch(sample))[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "T2Z7RR6db9md",
        "outputId": "4779dd90-2352-4a1f-b8c1-269820647f23"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-f4301f780bc8>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    dataset_size = #enter dataset size\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prior(kernel_size, bias_size, dtype=None):\n",
        "    n = kernel_size + bias_size\n",
        "    prior_model = keras.Sequential(\n",
        "        [\n",
        "            tfp.layers.DistributionLambda(\n",
        "                lambda t: tfp.distributions.MultivariateNormalDiag(\n",
        "                    loc=tf.zeros(n), scale_diag=tf.ones(n)\n",
        "                )\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "    return prior_model\n",
        "\n",
        "def posterior(kernel_size, bias_size, dtype=None):\n",
        "    n = kernel_size + bias_size\n",
        "    posterior_model = keras.Sequential(\n",
        "        [\n",
        "            tfp.layers.VariableLayer(\n",
        "                tfp.layers.MultivariateNormalTriL.params_size(n), dtype=dtype\n",
        "            ),\n",
        "            tfp.layers.MultivariateNormalTriL(n),\n",
        "        ]\n",
        "    )\n",
        "    return posterior_model"
      ],
      "metadata": {
        "id": "rJajxfGlcm0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bnn_model(train_size):\n",
        "    inputs = create_model_inputs()\n",
        "    features = keras.layers.concatenate(list(inputs.values()))\n",
        "    features = layers.BatchNormalization()(features)\n",
        "\n",
        "    # Create hidden layers with weight uncertainty using the DenseVariational layer.\n",
        "    for units in hidden_units:\n",
        "        features = tfp.layers.DenseVariational(\n",
        "            units=units,\n",
        "            make_prior_fn=prior,\n",
        "            make_posterior_fn=posterior,\n",
        "            kl_weight=1 / train_size,\n",
        "            activation=\"sigmoid\",\n",
        "        )(features)\n",
        "\n",
        "    # The output is deterministic: a single point estimate.\n",
        "    outputs = layers.Dense(units=1)(features)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "def compute_predictions(model, iterations=100):\n",
        "    predicted = []\n",
        "    for _ in range(iterations):\n",
        "        predicted.append(model(examples).numpy())\n",
        "    predicted = np.concatenate(predicted, axis=1)\n",
        "\n",
        "    prediction_mean = np.mean(predicted, axis=1).tolist()\n",
        "    prediction_min = np.min(predicted, axis=1).tolist()\n",
        "    prediction_max = np.max(predicted, axis=1).tolist()\n",
        "    prediction_range = (np.max(predicted, axis=1) - np.min(predicted, axis=1)).tolist()\n",
        "\n",
        "    for idx in range(sample):\n",
        "        print(\n",
        "            f\"Predictions mean: {round(prediction_mean[idx], 2)}, \"\n",
        "            f\"min: {round(prediction_min[idx], 2)}, \"\n",
        "            f\"max: {round(prediction_max[idx], 2)}, \"\n",
        "            f\"range: {round(prediction_range[idx], 2)} - \"\n",
        "            f\"Actual: {targets[idx]}\"\n",
        "        )\n"
      ],
      "metadata": {
        "id": "d_Ggb9pdesGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 500\n",
        "bnn_model_full = create_bnn_model(train_size)\n",
        "run_experiment(bnn_model_full, mse_loss, train_dataset, test_dataset)\n",
        "\n",
        "compute_predictions(bnn_model_full)"
      ],
      "metadata": {
        "id": "yHddZ727eyNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kl4NVnI7e1fS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}