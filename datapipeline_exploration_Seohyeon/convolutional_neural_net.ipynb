{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzIGA4tP2+uSkXMTiXH3ih",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/p-ai-org/p-music/blob/main/datapipeline_exploration_Seohyeon/convolutional_neural_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "80% training and 20% validation of outermost arrays for both X and y. \n",
        "X: spectrogram dataset (go thru all the folders), y: song rating (find that folder)"
      ],
      "metadata": {
        "id": "ab2BH_HZv3zH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GGxeNFxan0I",
        "outputId": "32bf51b0-af58-4f8d-b97b-b64c25f0a01b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.7/dist-packages (0.17.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (0.1.7)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (1.5.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (4.4.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (1.21.6)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (1.3.0)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (0.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (4.6.0)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (3.19.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (4.64.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.21.6)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.10.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (4.1.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (5.10.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.3.6)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.9.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (1.24.3)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.7/dist-packages (from etils[epath]->tensorflow-datasets) (3.10.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.56.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-probability\n",
        "!pip install tensorflow-datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp"
      ],
      "metadata": {
        "id": "5I2GQLV-a686"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dimension of spectrograms\n",
        "image_size = (1001,512)\n",
        "batch_size = 128\n",
        "\n",
        "#process spectrograms\n",
        "#is this step necessary?\n",
        "train_ds, val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"enter directory\",\n",
        "    validation_split=0.2,\n",
        "    #return both train and val datasets\n",
        "    subset=\"both\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        ")"
      ],
      "metadata": {
        "id": "6JH_u-OxhlRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # We shuffle with a buffer the same size as the dataset.\n",
        "train_dataset = (\n",
        "        train_ds.take(train_size).shuffle(buffer_size=train_size).batch(batch_size)\n",
        "    )\n",
        "    test_dataset = dataset.skip(train_size).batch(batch_size)\n",
        "\n",
        "    return train_dataset, test_dataset\n"
      ],
      "metadata": {
        "id": "Vr-Q6y1DbJeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(model, loss, train_dataset, test_dataset):\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
        "        loss=loss,\n",
        "        metrics=[keras.metrics.RootMeanSquaredError()],\n",
        "    )\n",
        "\n",
        "    print(\"Start training the model...\")\n",
        "    model.fit(train_dataset, epochs=num_epochs, validation_data=test_dataset)\n",
        "    print(\"Model training finished.\")\n",
        "    _, rmse = model.evaluate(train_dataset, verbose=0)\n",
        "    print(f\"Train RMSE: {round(rmse, 3)}\")\n",
        "\n",
        "    print(\"Evaluating model performance...\")\n",
        "    _, rmse = model.evaluate(test_dataset, verbose=0)\n",
        "    print(f\"Test RMSE: {round(rmse, 3)}\")"
      ],
      "metadata": {
        "id": "CKk08Mu0b89O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spectrograms 2262*512 or 1001*512\n",
        "dataset_size = #enter dataset size\n",
        "batch_size = 256\n",
        "num_epochs = 100\n",
        "sample = 10\n",
        "train_size = int(dataset_size * 0.85)\n",
        "\n",
        "mse_loss = keras.losses.MeanSquaredError()\n",
        "\n",
        "# Split the datasets into two groups\n",
        "train_dataset, test_dataset = get_train_and_test_splits(train_size, batch_size)\n",
        "\n",
        "# Sample some example targets from the test dataset\n",
        "examples, targets = list(test_dataset.unbatch().shuffle(batch_size * 10).batch(sample))[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "T2Z7RR6db9md",
        "outputId": "4779dd90-2352-4a1f-b8c1-269820647f23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-f4301f780bc8>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    dataset_size = #enter dataset size\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
      ],
      "metadata": {
        "id": "rJajxfGlcm0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_images, train_labels, epochs=10, \n",
        "                    validation_data=(test_images, test_labels))"
      ],
      "metadata": {
        "id": "d_Ggb9pdesGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yHddZ727eyNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kl4NVnI7e1fS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}